{"cells":[{"cell_type":"code","source":["%pip install mlflow-skinny==1.28"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db62b738-11b5-4050-b5d8-e20afe05c779"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nCollecting mlflow-skinny==1.28\n  Downloading mlflow_skinny-1.28.0-py3-none-any.whl (3.5 MB)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (3.1.27)\nRequirement already satisfied: cloudpickle<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2.0.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (3.19.4)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (6.0)\nRequirement already satisfied: pytz<2023 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2021.3)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2.26.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<5,>=3.7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (4.8.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.17.0)\nRequirement already satisfied: packaging<22 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (21.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (8.0.3)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (2.4.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow-skinny==1.28) (4.0.9)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow-skinny==1.28) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<5,>=3.7.0->mlflow-skinny==1.28) (3.6.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging<22->mlflow-skinny==1.28) (3.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (2021.10.8)\nInstalling collected packages: mlflow-skinny\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 1.27.0\n    Uninstalling mlflow-skinny-1.27.0:\n      Successfully uninstalled mlflow-skinny-1.27.0\nSuccessfully installed mlflow-skinny-1.28.0\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nCollecting mlflow-skinny==1.28\n  Downloading mlflow_skinny-1.28.0-py3-none-any.whl (3.5 MB)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (3.1.27)\nRequirement already satisfied: cloudpickle<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2.0.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (3.19.4)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (6.0)\nRequirement already satisfied: pytz<2023 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2021.3)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (2.26.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<5,>=3.7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (4.8.1)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.4.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (0.17.0)\nRequirement already satisfied: packaging<22 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (21.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow-skinny==1.28) (8.0.3)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28) (2.4.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitpython<4,>=2.1.0->mlflow-skinny==1.28) (4.0.9)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow-skinny==1.28) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.9/site-packages (from importlib-metadata!=4.7.0,<5,>=3.7.0->mlflow-skinny==1.28) (3.6.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging<22->mlflow-skinny==1.28) (3.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow-skinny==1.28) (2021.10.8)\nInstalling collected packages: mlflow-skinny\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 1.27.0\n    Uninstalling mlflow-skinny-1.27.0:\n      Successfully uninstalled mlflow-skinny-1.27.0\nSuccessfully installed mlflow-skinny-1.28.0\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["from typing import List\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\ndata: List = [\n              (1, 3, 1),\n  (1, 3, 1),\n  (1, 3, 1),\n  (1, 3, 1),\n  (1, 3, 1),\n              (2, 4, 1),\n              (2, 3, 1),\n              (3, 3, 1),\n              (3, 4, 1)\n  ]\n\nschema: StructType = StructType([ \\\n    StructField(\"utilisateur_identifiant\", IntegerType(), True), \\\n    StructField(\"diamant_identifiant\", IntegerType(), True),\n    StructField(\"nombre_de_fois_achetes\", IntegerType(), True)\n  ])\n\ndiamants_pre_features: DataFrame = spark.createDataFrame(data=data,schema=schema)\n\n\nfrom pyspark.ml.recommendation import ALS, ALSModel\n\nals: ALS = ALS(\n  userCol=\"utilisateur_identifiant\", \n  itemCol=\"diamant_identifiant\", \n  ratingCol=\"nombre_de_fois_achetes\",\n  implicitPrefs=True,\n  alpha=40,\n  nonnegative=True\n)\nmodel: ALSModel = als.fit(diamants_pre_features)\n\nimport mlflow\nmlflow.set_experiment(\"/nastasia/ALS_experiment\")\n\nwith mlflow.start_run() as last_run:\n  mlflow.spark.log_model(model, \"als_exp\")\n\nfrom mlflow.tracking import MlflowClient\n# Get last run from Mlflow experiment\nclient = MlflowClient()\n\nmodel_experiment_id = client.get_experiment_by_name(\"/nastasia/ALS_experiment\").experiment_id\n\nruns = client.search_runs(\n        model_experiment_id, order_by=[\"start_time DESC\"]\n)\n\nrun_uuid = runs[0].info.run_uuid\n\n# can be loaded from s3\n# model = ALSModel.load(sources_jobs['ALS_model'])\nloaded_model = mlflow.spark.load_model(f\"runs:/{run_uuid}/als_exp\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd313d76-3f97-4779-8090-dd4cf6407e74"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"2022/10/03 11:44:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmplav5lz8q, flavor: spark), fall back to return ['pyspark==3.3.0']. Set logging level to DEBUG to see the full traceback.\n2022/10/03 11:44:39 INFO mlflow.spark: 'runs:/e66f017a224a45f5bd9af3510d85d5da/als_exp' resolved as 'dbfs:/databricks/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp'\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["2022/10/03 11:44:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmplav5lz8q, flavor: spark), fall back to return ['pyspark==3.3.0']. Set logging level to DEBUG to see the full traceback.\n2022/10/03 11:44:39 INFO mlflow.spark: 'runs:/e66f017a224a45f5bd9af3510d85d5da/als_exp' resolved as 'dbfs:/databricks/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp'\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-584661241112111>\u001B[0m in \u001B[0;36m<cell line: 58>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;31m# can be loaded from s3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;31m# model = ALSModel.load(sources_jobs['ALS_model'])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m \u001B[0mloaded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmlflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"runs:/{run_uuid}/als_exp\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3c21b356-9cd5-407f-99c9-99eaac466b0b/lib/python3.9/site-packages/mlflow/spark.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir)\u001B[0m\n\u001B[1;32m    784\u001B[0m             \u001B[0mget_databricks_profile_uri_from_artifact_uri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot_uri\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m         ):\n\u001B[0;32m--> 786\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPipelineModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmlflowdbfs_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    787\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m     return _load_model(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/util.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(cls, path)\u001B[0m\n\u001B[1;32m    444\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mRL\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m         \u001B[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0mmetadata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDefaultParamsReader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadMetadata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"language\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"paramMap\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"paramMap\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"language\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"Python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mJavaMLReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mType\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"JavaMLReadable[PipelineModel]\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m             \u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipelineSharedReadWrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/util.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    393\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"path should be a string, got type %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 395\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    396\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_clazz\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_from_java\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m             raise NotImplementedError(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o557.load.\n: java.lang.AssertionError: assertion failed: Conflicting partition column names detected:\n\n\tPartition column name list #0: part-00000-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1446-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #1: part-00001-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1447-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #2: part-00002-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1448-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #3: part-00003-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1449-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\nFor partitioned table directories, data files should only live in leaf directories.\nAnd directories at the same level should have the same partition column name.\nPlease check the following directories for unexpected files or inconsistent partition column names:\n\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00000-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1446-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=86bc1d071902ae543c8e3236885a2117f889caf4922374bddd32ebf270b35f93/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=589\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00001-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1447-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=c68d5037c4cbc8a7fac3668b2bac9a83af4ed7872529c5b87b87163b38ed001f/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=946\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00002-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1448-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=8ad7a2f178c942e6aa7adaca3368053a76b1acc0914808ca164f5643ac5a0624/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=946\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00003-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1449-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=41fa1679edb110b03ebcc4a9f3124ca7d9b1825aad2f37fe1941a87a80f8ca62/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=945\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:482)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:213)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:142)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:212)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:106)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:368)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:324)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:324)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:558)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:548)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"java.lang.AssertionError: assertion failed: Conflicting partition column names detected:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-584661241112111>\u001B[0m in \u001B[0;36m<cell line: 58>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;31m# can be loaded from s3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[0;31m# model = ALSModel.load(sources_jobs['ALS_model'])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m \u001B[0mloaded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmlflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"runs:/{run_uuid}/als_exp\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-3c21b356-9cd5-407f-99c9-99eaac466b0b/lib/python3.9/site-packages/mlflow/spark.py\u001B[0m in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir)\u001B[0m\n\u001B[1;32m    784\u001B[0m             \u001B[0mget_databricks_profile_uri_from_artifact_uri\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot_uri\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m         ):\n\u001B[0;32m--> 786\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mPipelineModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmlflowdbfs_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    787\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m     return _load_model(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/util.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(cls, path)\u001B[0m\n\u001B[1;32m    444\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mRL\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m         \u001B[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    447\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    282\u001B[0m         \u001B[0mmetadata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDefaultParamsReader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloadMetadata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    283\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m\"language\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"paramMap\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"paramMap\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"language\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"Python\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 284\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mJavaMLReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mType\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"JavaMLReadable[PipelineModel]\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    285\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    286\u001B[0m             \u001B[0muid\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipelineSharedReadWrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/util.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    393\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    394\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"path should be a string, got type %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 395\u001B[0;31m         \u001B[0mjava_obj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    396\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_clazz\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_from_java\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m             raise NotImplementedError(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o557.load.\n: java.lang.AssertionError: assertion failed: Conflicting partition column names detected:\n\n\tPartition column name list #0: part-00000-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1446-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #1: part-00001-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1447-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #2: part-00002-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1448-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\tPartition column name list #3: part-00003-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1449-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\n\nFor partitioned table directories, data files should only live in leaf directories.\nAnd directories at the same level should have the same partition column name.\nPlease check the following directories for unexpected files or inconsistent partition column names:\n\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00000-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1446-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=86bc1d071902ae543c8e3236885a2117f889caf4922374bddd32ebf270b35f93/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=589\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00001-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1447-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=c68d5037c4cbc8a7fac3668b2bac9a83af4ed7872529c5b87b87163b38ed001f/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=946\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00002-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1448-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=8ad7a2f178c942e6aa7adaca3368053a76b1acc0914808ca164f5643ac5a0624/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=946\n\tmlflowdbfs:/databricks-prod-storage-oregon.s3.us-west-2.amazonaws.com/ce/7537163453515898.jobs/mlflow-tracking/584661241112115/e66f017a224a45f5bd9af3510d85d5da/artifacts/als_exp/sparkml/stages/0_ALS_bc3f8e95f226/userFactors/part-00003-tid-4099602714812222827-3102ccf6-b97a-40a9-ace9-0ba1dd5766dd-1449-1-c000.snappy.parquet?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDnVzLXdlc3QtMi1maXBzIkgwRgIhAO5aCKqHZJXAGDDLZZ0dYw9myP%2F6ZtU7i2GceVCw1SSyAiEAoeSsgWTwFuMQlD5H0mz43Lo1tCBCZ3jEN0iQcYIm2hcqtgIIpf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw0MTQzNTE3Njc4MjYiDOgXjrZhNfFdcDLcniqKAkVi8B1TcVc7wwLsVmfXjA%2F8tDEriBh3%2F%2BgqHr8m%2FS1Elb%2FLP4r6nnVoXUeAxelAlvh5zlQgGY06Vie6J7B1COX%2FgecEIIqkS%2FTAQY%2BS%2FaculPS%2BRQjrD391Qr2YJuuKNzFKrJXWMeXTYa7x%2Bp40HJykLJvHJAusyii0YgQ9yr1mD7bdpzVoA8R3zoxn0pu2%2Be4gBIVz6DA1idFJF4XJBx%2BAj7HiNYMnS8JkbmJCGGtheQIufn1wC979ZUE6RAutIGu3KnaG25j%2BAZiWQfRaLxx7twue%2B%2FK0JPXYWDwJljmKu8mA3PzxGSaoWj9pFTqlLxWKNiwJX9OljKqZMEtGs%2BMWXthtowwpffcSMI%2BO65kGOpwBGiUvyg1Lk6Ga%2F41bw8%2Fj9zwk8P0YkR6%2B28D4q4DvE4oiJB9WR%2FguDUcAZZVluDgs0JKTP%2BV13LtTySVXD%2FmtFPNYo8IxWNRb7aI2wPqt%2FgTS%2BZOWjyKn064AVESjANBNMHl3UdfkCHPUq7NofEpbi%2FyLUGDwBOe6bybT8bGh%2FawrznShI5oTwPTg45Nn7gd2AUALlchtML5XS8QI&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221003T114446Z&X-Amz-SignedHeaders=host&X-Amz-Expires=899&X-Amz-Credential=ASIAWA6KKHEJJFM5QVR2%2F20221003%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=41fa1679edb110b03ebcc4a9f3124ca7d9b1825aad2f37fe1941a87a80f8ca62/_cloud_type_=AWS_PRESIGNED_URL/_file_size_=945\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:482)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:213)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:142)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:212)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:106)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:53)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:368)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:324)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:324)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:558)\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:548)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}}],"execution_count":0},{"cell_type":"code","source":["%sh pip freeze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20b573a7-fe9a-4366-b17b-5f06574f58fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"absl-py==1.0.0\nargon2-cffi==20.1.0\nastor==0.8.1\nastunparse==1.6.3\nasync-generator==1.10\nattrs==21.2.0\nazure-core==1.22.1\nazure-cosmos==4.2.0\nbackcall==0.2.0\nbackports.entry-points-selectable==1.1.1\nbcrypt==4.0.0\nblack==22.3.0\nbleach==4.0.0\nblis==0.7.8\nboto3==1.21.18\nbotocore==1.24.18\ncachetools==5.2.0\ncatalogue==2.0.8\ncertifi==2021.10.8\ncffi==1.14.6\nchardet==4.0.0\ncharset-normalizer==2.0.4\nclick==8.0.3\ncloudpickle==2.0.0\ncmdstanpy==0.9.68\nconfigparser==5.2.0\nconvertdate==2.4.0\ncryptography==3.4.8\ncycler==0.10.0\ncymem==2.0.6\nCython==0.29.24\ndatabricks-automl-runtime==0.2.10\ndatabricks-cli==0.17.0\ndbl-tempo==0.1.12\ndbus-python==1.2.16\ndebugpy==1.4.1\ndecorator==5.1.0\ndefusedxml==0.7.1\ndill==0.3.4\ndiskcache==5.4.0\ndistlib==0.3.5\ndistro==1.4.0\ndistro-info===0.23ubuntu1\nentrypoints==0.3\nephem==4.1.3\nfacets-overview==1.0.0\nfasttext==0.9.2\nfilelock==3.3.1\nFlask==1.1.2\nflatbuffers==1.12\nfsspec==2021.8.1\nfuture==0.18.2\ngast==0.4.0\ngitdb==4.0.9\nGitPython==3.1.27\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.44.0\ngunicorn==20.1.0\ngviz-api==1.10.0\nh5py==3.3.0\nhijri-converter==2.2.4\nholidays==0.14.2\nhorovod==0.25.0\nhtmlmin==0.1.12\nhuggingface-hub==0.9.0\nidna==3.2\nImageHash==4.2.1\nimbalanced-learn==0.8.1\nimportlib-metadata==4.8.1\nipykernel==6.12.1\nipython==7.32.0\nipython-genutils==0.2.0\nipywidgets==7.7.0\nisodate==0.6.1\nitsdangerous==2.0.1\njedi==0.18.0\nJinja2==2.11.3\njmespath==0.10.0\njoblib==1.0.1\njoblibspark==0.5.0\njsonschema==3.2.0\njupyter-client==6.1.12\njupyter-core==4.8.1\njupyterlab-pygments==0.1.2\njupyterlab-widgets==1.0.0\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkiwisolver==1.3.1\nkorean-lunar-calendar==0.2.1\nlangcodes==3.3.0\nlibclang==14.0.6\nlightgbm==3.3.2\nllvmlite==0.37.0\nLunarCalendar==0.0.9\nMako==1.2.0\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nmatplotlib==3.4.3\nmatplotlib-inline==0.1.2\nmissingno==0.5.1\nmistune==0.8.4\nmleap==0.20.0\nmlflow-skinny==1.28.0\nmultimethod==1.8\nmurmurhash==1.0.8\nmypy-extensions==0.4.3\nnbclient==0.5.3\nnbconvert==6.1.0\nnbformat==5.1.3\nnest-asyncio==1.5.1\nnetworkx==2.6.3\nnltk==3.6.5\nnotebook==6.4.5\nnumba==0.54.1\nnumpy==1.20.3\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.0\npandas==1.3.4\npandas-profiling==3.1.0\npandocfilters==1.4.3\nparamiko==2.9.2\nparso==0.8.2\npathspec==0.9.0\npathy==0.6.2\npatsy==0.5.2\npetastorm==0.11.4\npexpect==4.8.0\nphik==0.12.2\npickleshare==0.7.5\nPillow==8.4.0\nplatformdirs==2.5.2\nplotly==5.9.0\npmdarima==1.8.5\npreshed==3.0.7\nprometheus-client==0.11.0\nprompt-toolkit==3.0.20\nprophet==1.0.1\nprotobuf==3.19.4\npsutil==5.8.0\npsycopg2==2.9.3\nptyprocess==0.7.0\npyarrow==7.0.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\npybind11==2.10.0\npycparser==2.20\npydantic==1.9.2\nPygments==2.10.0\nPyGObject==3.36.0\nPyJWT==2.4.0\nPyMeeus==0.5.11\nPyNaCl==1.5.0\npyodbc==4.0.31\npyparsing==3.0.4\npyrsistent==0.18.0\npystan==2.19.1.1\npython-apt==2.0.0+ubuntu0.20.4.8\npython-dateutil==2.8.2\npython-editor==1.0.4\npytz==2021.3\nPyWavelets==1.1.1\nPyYAML==6.0\npyzmq==22.2.1\nregex==2021.8.3\nrequests==2.26.0\nrequests-oauthlib==1.3.1\nrequests-unixsocket==0.2.0\nrsa==4.9\ns3transfer==0.5.2\nscikit-learn==0.24.2\nscipy==1.7.1\nseaborn==0.11.2\nSend2Trash==1.8.0\nsetuptools-git==1.2\nshap==0.41.0\nsimplejson==3.17.6\nsix==1.16.0\nslicer==0.0.7\nsmart-open==5.2.1\nsmmap==5.0.0\nspacy==3.4.0\nspacy-legacy==3.0.10\nspacy-loggers==1.0.3\nspark-tensorflow-distributor==1.0.0\nsqlparse==0.4.2\nsrsly==2.4.4\nssh-import-id==5.10\nstatsmodels==0.12.2\ntabulate==0.8.9\ntangled-up-in-unicode==0.1.0\ntenacity==8.0.1\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-profile==2.8.0\ntensorboard-plugin-wit==1.8.1\ntensorflow-cpu==2.9.1\ntensorflow-estimator==2.9.0\ntensorflow-io-gcs-filesystem==0.26.0\ntermcolor==1.1.0\nterminado==0.9.4\ntestpath==0.5.0\nthinc==8.1.0\nthreadpoolctl==2.2.0\ntokenize-rt==4.2.1\ntokenizers==0.12.1\ntomli==2.0.1\ntorch==1.11.0+cpu\ntorchvision==0.12.0+cpu\ntornado==6.1\ntqdm==4.62.3\ntraitlets==5.1.0\ntransformers==4.20.1\ntyper==0.4.2\ntyping-extensions==3.10.0.2\nujson==4.0.2\nunattended-upgrades==0.1\nurllib3==1.26.7\nvirtualenv==20.8.0\nvisions==0.7.4\nwasabi==0.10.1\nwcwidth==0.2.5\nwebencodings==0.5.1\nwebsocket-client==1.3.1\nWerkzeug==2.0.2\nwidgetsnbextension==3.6.0\nwrapt==1.12.1\nxgboost==1.5.2\nzipp==3.6.0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["absl-py==1.0.0\nargon2-cffi==20.1.0\nastor==0.8.1\nastunparse==1.6.3\nasync-generator==1.10\nattrs==21.2.0\nazure-core==1.22.1\nazure-cosmos==4.2.0\nbackcall==0.2.0\nbackports.entry-points-selectable==1.1.1\nbcrypt==4.0.0\nblack==22.3.0\nbleach==4.0.0\nblis==0.7.8\nboto3==1.21.18\nbotocore==1.24.18\ncachetools==5.2.0\ncatalogue==2.0.8\ncertifi==2021.10.8\ncffi==1.14.6\nchardet==4.0.0\ncharset-normalizer==2.0.4\nclick==8.0.3\ncloudpickle==2.0.0\ncmdstanpy==0.9.68\nconfigparser==5.2.0\nconvertdate==2.4.0\ncryptography==3.4.8\ncycler==0.10.0\ncymem==2.0.6\nCython==0.29.24\ndatabricks-automl-runtime==0.2.10\ndatabricks-cli==0.17.0\ndbl-tempo==0.1.12\ndbus-python==1.2.16\ndebugpy==1.4.1\ndecorator==5.1.0\ndefusedxml==0.7.1\ndill==0.3.4\ndiskcache==5.4.0\ndistlib==0.3.5\ndistro==1.4.0\ndistro-info===0.23ubuntu1\nentrypoints==0.3\nephem==4.1.3\nfacets-overview==1.0.0\nfasttext==0.9.2\nfilelock==3.3.1\nFlask==1.1.2\nflatbuffers==1.12\nfsspec==2021.8.1\nfuture==0.18.2\ngast==0.4.0\ngitdb==4.0.9\nGitPython==3.1.27\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.44.0\ngunicorn==20.1.0\ngviz-api==1.10.0\nh5py==3.3.0\nhijri-converter==2.2.4\nholidays==0.14.2\nhorovod==0.25.0\nhtmlmin==0.1.12\nhuggingface-hub==0.9.0\nidna==3.2\nImageHash==4.2.1\nimbalanced-learn==0.8.1\nimportlib-metadata==4.8.1\nipykernel==6.12.1\nipython==7.32.0\nipython-genutils==0.2.0\nipywidgets==7.7.0\nisodate==0.6.1\nitsdangerous==2.0.1\njedi==0.18.0\nJinja2==2.11.3\njmespath==0.10.0\njoblib==1.0.1\njoblibspark==0.5.0\njsonschema==3.2.0\njupyter-client==6.1.12\njupyter-core==4.8.1\njupyterlab-pygments==0.1.2\njupyterlab-widgets==1.0.0\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkiwisolver==1.3.1\nkorean-lunar-calendar==0.2.1\nlangcodes==3.3.0\nlibclang==14.0.6\nlightgbm==3.3.2\nllvmlite==0.37.0\nLunarCalendar==0.0.9\nMako==1.2.0\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nmatplotlib==3.4.3\nmatplotlib-inline==0.1.2\nmissingno==0.5.1\nmistune==0.8.4\nmleap==0.20.0\nmlflow-skinny==1.28.0\nmultimethod==1.8\nmurmurhash==1.0.8\nmypy-extensions==0.4.3\nnbclient==0.5.3\nnbconvert==6.1.0\nnbformat==5.1.3\nnest-asyncio==1.5.1\nnetworkx==2.6.3\nnltk==3.6.5\nnotebook==6.4.5\nnumba==0.54.1\nnumpy==1.20.3\noauthlib==3.2.0\nopt-einsum==3.3.0\npackaging==21.0\npandas==1.3.4\npandas-profiling==3.1.0\npandocfilters==1.4.3\nparamiko==2.9.2\nparso==0.8.2\npathspec==0.9.0\npathy==0.6.2\npatsy==0.5.2\npetastorm==0.11.4\npexpect==4.8.0\nphik==0.12.2\npickleshare==0.7.5\nPillow==8.4.0\nplatformdirs==2.5.2\nplotly==5.9.0\npmdarima==1.8.5\npreshed==3.0.7\nprometheus-client==0.11.0\nprompt-toolkit==3.0.20\nprophet==1.0.1\nprotobuf==3.19.4\npsutil==5.8.0\npsycopg2==2.9.3\nptyprocess==0.7.0\npyarrow==7.0.0\npyasn1==0.4.8\npyasn1-modules==0.2.8\npybind11==2.10.0\npycparser==2.20\npydantic==1.9.2\nPygments==2.10.0\nPyGObject==3.36.0\nPyJWT==2.4.0\nPyMeeus==0.5.11\nPyNaCl==1.5.0\npyodbc==4.0.31\npyparsing==3.0.4\npyrsistent==0.18.0\npystan==2.19.1.1\npython-apt==2.0.0+ubuntu0.20.4.8\npython-dateutil==2.8.2\npython-editor==1.0.4\npytz==2021.3\nPyWavelets==1.1.1\nPyYAML==6.0\npyzmq==22.2.1\nregex==2021.8.3\nrequests==2.26.0\nrequests-oauthlib==1.3.1\nrequests-unixsocket==0.2.0\nrsa==4.9\ns3transfer==0.5.2\nscikit-learn==0.24.2\nscipy==1.7.1\nseaborn==0.11.2\nSend2Trash==1.8.0\nsetuptools-git==1.2\nshap==0.41.0\nsimplejson==3.17.6\nsix==1.16.0\nslicer==0.0.7\nsmart-open==5.2.1\nsmmap==5.0.0\nspacy==3.4.0\nspacy-legacy==3.0.10\nspacy-loggers==1.0.3\nspark-tensorflow-distributor==1.0.0\nsqlparse==0.4.2\nsrsly==2.4.4\nssh-import-id==5.10\nstatsmodels==0.12.2\ntabulate==0.8.9\ntangled-up-in-unicode==0.1.0\ntenacity==8.0.1\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-profile==2.8.0\ntensorboard-plugin-wit==1.8.1\ntensorflow-cpu==2.9.1\ntensorflow-estimator==2.9.0\ntensorflow-io-gcs-filesystem==0.26.0\ntermcolor==1.1.0\nterminado==0.9.4\ntestpath==0.5.0\nthinc==8.1.0\nthreadpoolctl==2.2.0\ntokenize-rt==4.2.1\ntokenizers==0.12.1\ntomli==2.0.1\ntorch==1.11.0+cpu\ntorchvision==0.12.0+cpu\ntornado==6.1\ntqdm==4.62.3\ntraitlets==5.1.0\ntransformers==4.20.1\ntyper==0.4.2\ntyping-extensions==3.10.0.2\nujson==4.0.2\nunattended-upgrades==0.1\nurllib3==1.26.7\nvirtualenv==20.8.0\nvisions==0.7.4\nwasabi==0.10.1\nwcwidth==0.2.5\nwebencodings==0.5.1\nwebsocket-client==1.3.1\nWerkzeug==2.0.2\nwidgetsnbextension==3.6.0\nwrapt==1.12.1\nxgboost==1.5.2\nzipp==3.6.0\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b2f798f-4237-4c54-8688-92995c0306a8"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Test","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":584661241112110}},"nbformat":4,"nbformat_minor":0}
